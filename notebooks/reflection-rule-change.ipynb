{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Changing the rules about windows making reflections\n",
    "\n",
    "This notebook demonstrates a rank-one change that reverses a nontrivial rule that connects the presence of windows with the appearance of countertop reflections.  When the rank-one change is made in layer 6, adding a window will remove a reflection instead of adding it.\n",
    "\n",
    "To investigate this we proceed in several steps.\n",
    "\n",
    "1. We loada pretrained progressive GAN for kitchen scenes, and then use GAN dissection to identify units in layer4 that control the presence of windows in the scene.  This analysis is preloaded.\n",
    "2. We choose a set of generated scenes that have windows or countertops, and we use an interactive tool to create masks that activate or deactivate units to add and remove windows in those scene.  Then using the same tool, we identify reflections on countertops that are affected when windows are added or removed.\n",
    "3. To identiify if any layer is decisive in connecting reflections to windows, we pass through layers 1 through 8, and use optimization to try to find a change in the layer which reverses the reflections (darkening countertops when windows are added and vice-versa.)  We notice that a change in layer6 achieves this change best.  Furthermore, (not shown here), we notice that the change is very close to a rank-one change.\n",
    "4. To find an exactly rank-one change to layer6, we repeat the optimization to flip the reflection rule, but project to a rank-one change (using SVD) periodically during the optimization.\n",
    "5. To demonstrate the rule change, we show an interactive tool that allows windows to be added or removed in sches generated by both the original unchanged model, and the new model where the reflection rule is flipped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load a pretrained kitchen progressive GAN\n",
    "\n",
    "Also load a precomputed dissection that identifies units responsbile for windows in layer4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load a kitchen model and dissection.\n",
    "\n",
    "import torch, copy, os, json\n",
    "from utils import nethook, proggan, zdataset, imgviz, show, upsample, tally, proggan\n",
    "from utils import renormalize, quickdissect\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "modelname = 'kitchen'\n",
    "dirname = 'masks/reflections/progan-%s' % modelname\n",
    "dis = quickdissect.DissectVis(model=modelname)\n",
    "device = torch.device('cuda:0')\n",
    "model = proggan.load_pretrained('kitchen')\n",
    "model.cuda()\n",
    "zds = zdataset.z_dataset_for_model(model, size=1000)\n",
    "os.makedirs(dirname, exist_ok=True)\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "def rf(fn):\n",
    "    return os.path.join(dirname, fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the model to generate 30 images that happen to have reflective countertops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reflective_imgnums = sorted([\n",
    "    958, 505, 487, 229, 723, 777, 819, 133,\n",
    "    285, 275, 246, 594, 244, 21, 524, 770, 788,\n",
    "    947, 350, 587, 889, 14, 38, 46, 52,\n",
    "    126, 147, 186, 196, 33\n",
    "])\n",
    "zbatch = torch.cat([zds[i][0][None] for i in reflective_imgnums]).cuda()\n",
    "imgbatch = model(zbatch)\n",
    "iv = imgviz.ImageVisualizer(140)\n",
    "\n",
    "show([[i, iv.image(imgbatch[j])] for j, i in enumerate(reflective_imgnums)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Interactively create masks for adding and removing windows\n",
    "\n",
    "The tool allows a user to point at two things:\n",
    "1. locations where a window should be added or removed\n",
    "2. locations of shiny countertops that show reflections that change in response to the window.\n",
    "\n",
    "This tool is used to save or load a set of such masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Collect and freeze a set of interventions both to add and to remove windows.\n",
    "from utils import labwidget, paintwidget\n",
    "\n",
    "class InteractionProber(labwidget.Widget):\n",
    "    def __init__(self, model, state=None):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.state = {int(k): v for k,v in state.items()} if state else {}\n",
    "        self.orig = labwidget.Div()\n",
    "        self.menu = labwidget.Menu(choices=EXAMPLES).on(\n",
    "            'selection', self.set_selection)\n",
    "        self.valuebox = labwidget.Textbox(10.0).on(\n",
    "            'value', self.rerender)\n",
    "        self.intervention = paintwidget.PaintWidget().on(\n",
    "            'mask', self.rerender)\n",
    "        self.ibutton = labwidget.Button('clear edit').on(\n",
    "            'click', self.clear_intervention)\n",
    "        self.revert = paintwidget.PaintWidget().on(\n",
    "            'mask', self.rerender)\n",
    "        self.rbutton = labwidget.Button('clear mask').on(\n",
    "            'click', self.clear_revert)\n",
    "        self.menu.selection = EXAMPLES[0]\n",
    "        # self.rerender(None)\n",
    "    \n",
    "    def set_selection(self):\n",
    "        imgnum = int(self.menu.selection)\n",
    "        if imgnum in self.state:\n",
    "            record = self.state[imgnum]\n",
    "            self.valuebox.value, self.intervention.mask, self.revert.mask = [\n",
    "                record[k] for k in 'value intervention revert'.split()]\n",
    "        else:\n",
    "            self.valuebox.value, self.intervention.mask, self.revert.mask = [\n",
    "                '10.0', '', ''\n",
    "            ]\n",
    "        self.rerender()\n",
    "        \n",
    "    def rerender(self):\n",
    "        imgnum = int(self.menu.selection)\n",
    "        value = float(self.valuebox.value)\n",
    "        self.state[imgnum] = dict(\n",
    "            value=self.valuebox.value,\n",
    "            intervention=self.intervention.mask,\n",
    "            revert=self.revert.mask)\n",
    "        self.orig.show(renormalize.as_image(self.render_result(\n",
    "                imgnum, value, None, None)))\n",
    "        self.intervention.image = renormalize.as_url(self.render_result(\n",
    "                imgnum, value, self.intervention.mask, None))\n",
    "        self.revert.image = renormalize.as_url(self.render_result(\n",
    "                imgnum, value, self.intervention.mask, self.revert.mask))\n",
    "        \n",
    "    def render_result(self, image_num, value, intervention_mask, revert_mask):\n",
    "        imodel = nethook.InstrumentedModel(copy.deepcopy(self.model))\n",
    "        layername = 'layer4'\n",
    "        imodel.retain_layer(layername)\n",
    "        z = zds[image_num][0][None].cuda()\n",
    "        orig_out = imodel(z)\n",
    "        if not intervention_mask:\n",
    "            return orig_out[0]\n",
    "        acts = imodel.retained_layer(layername)\n",
    "        units = dis.top_units(layername, 'window', 20)\n",
    "        iarea = renormalize.from_url(\n",
    "            intervention_mask, target='pt', size=acts.shape[2:])[0]\n",
    "        def editrule(x, name):\n",
    "            x[:,units] = (\n",
    "                value * iarea[None].to(x.device) +\n",
    "                x[:,units] * (1 - iarea[None].to(x.device))\n",
    "            )\n",
    "            return x\n",
    "        imodel.edit_layer(layername, rule=editrule)\n",
    "        inter_out = imodel(z)\n",
    "        if not revert_mask:\n",
    "            return inter_out[0]\n",
    "        rarea = renormalize.from_url(\n",
    "            revert_mask, target='pt', size=inter_out.shape[2:]\n",
    "        )[0][None][None].to(orig_out.device)\n",
    "        revert_out = (rarea * orig_out) + (1 - rarea) * inter_out\n",
    "        return revert_out[0]\n",
    "    \n",
    "    def clear_intervention(self):\n",
    "        self.intervention.mask = ''\n",
    "        self.rerender()\n",
    "\n",
    "    def clear_revert(self):\n",
    "        self.revert.mask = ''\n",
    "        self.rerender()\n",
    "        \n",
    "    def widget_html(self):\n",
    "        return show.html([[self.menu], ['intervention strength:'], [self.valuebox], [[self.orig], [self.intervention, self.ibutton], [self.revert, self.rbutton]]])\n",
    "\n",
    "EXAMPLES = reflective_imgnums\n",
    "\n",
    "with open(rf('posneg.json')) as f:\n",
    "    data = {int(k): v for k,v in json.load(f).items() if int(k) in EXAMPLES}\n",
    "\n",
    "prober = InteractionProber(model, data)\n",
    "show(prober)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three panes are shown in the tool above.  To use it:\n",
    "1. Seleect an image number.\n",
    "2. Choose a strength for windows to add or remove (positive to add, negative to remove).\n",
    "3. In the middle pane, draw windows where they should be added or removed.\n",
    "4. In the right pane, mark reflective countertops that change in response.\n",
    "\n",
    "The masks are stored by the tool, and the line of code below can be used to save the masks to a file for use in training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: # True to save changes\n",
    "    with open(rf('posneg.json'), 'w') as f:\n",
    "        json.dump(prober.state, f, indent=1)\n",
    "len(prober.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Use optimization to try to find a change in a layer which reverses the reflections\n",
    "\n",
    "Which layer is responsbible for the reflection rule?\n",
    "\n",
    "* To answer this, we ask: what rule would have had to change for all the highlighted reflections\n",
    "     in these cases to be dark in the first place, without changing the other parts of the image?\n",
    "* We ask this question by hunting for weight changes restricted to a single layer.\n",
    "* We repeat this for each layer to see if changes in one layer affect the reflections more than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingredients:\n",
    "print('zbatch', zbatch.shape, zbatch.device)\n",
    "print('EXAMPLES', EXAMPLES)\n",
    "print('data', sorted(data.keys()))\n",
    "print('data contents', data[next(iter(data))].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# More ingredients: get all layer activations when the interventions are\n",
    "# high and low and etc.\n",
    "\n",
    "def apply_window_edit(imodel, intervention, layername='layer4'):\n",
    "    if intervention is  None:\n",
    "        return \n",
    "    units = dis.top_units(layername, 'window', 20)\n",
    "    imodel.retain_layer(layername)\n",
    "    orig_out = imodel(zbatch[0][None])\n",
    "    acts = imodel.retained_layer(layername)\n",
    "    batch_area = torch.cat([\n",
    "        renormalize.from_url(\n",
    "            data[k]['intervention'], target='pt',\n",
    "            size=acts.shape[2:])[0][None,None]\n",
    "        for k in sorted(data.keys())]).to(acts.device)\n",
    "    def editrule(x, name):\n",
    "        x = x.clone()\n",
    "        x[:,units] = (\n",
    "            intervention * batch_area + x[:,units] * (1 - batch_area)\n",
    "        )\n",
    "        return x\n",
    "    imodel.edit_layer(layername, rule=editrule)\n",
    "        \n",
    "        \n",
    "# Some functions to run the network with various interventions.\n",
    "def render_with_change(intervention=None, return_layers=[]):\n",
    "    imodel = nethook.InstrumentedModel(copy.deepcopy(model))\n",
    "    layername = 'layer4'\n",
    "    imodel.retain_layers(return_layers)\n",
    "    apply_window_edit(imodel, intervention)\n",
    "    out = imodel(zbatch)\n",
    "    #if not revert_mask:\n",
    "    #    return inter_out[0]\n",
    "    # rarea = renormalize.from_url(\n",
    "    #     revert_mask, target='pt', size=inter_out.shape[2:]\n",
    "    # )[0][None][None].to(orig_out.device)\n",
    "    # revert_out = (rarea * orig_out) + (1 - rarea) * inter_out\n",
    "    results = imodel.retained_features()\n",
    "    results['x'] = out\n",
    "    return results\n",
    "\n",
    "ALL_LAYERS = ['layer%s' % i for i in range(1, 15)]\n",
    "orig_out = render_with_change(None, return_layers=ALL_LAYERS)\n",
    "high_out = render_with_change(10.0, return_layers=ALL_LAYERS)\n",
    "low_out = render_with_change(-5.0, return_layers=ALL_LAYERS)\n",
    "\n",
    "# Use this like\n",
    "# blended_layer10 = paste_acts(high_out['layer10'], low_out['layer10'], 'revert'/'intervention')\n",
    "\n",
    "def paste_acts(background, foreground, field):\n",
    "    batch_area = torch.cat([\n",
    "            renormalize.from_url(\n",
    "                data[k][field], target='pt',\n",
    "                size=foreground.shape[2:])[0][None,None]\n",
    "            for k in sorted(data.keys())]).to(foreground.device)\n",
    "    return (batch_area * foreground) + (1 - batch_area) * background    \n",
    "\n",
    "def render_from_layer(back_all, fore_all, field, layername):\n",
    "    rendering_model = nethook.subsequence(model, after_layer=layername)\n",
    "    batch_acts = paste_acts(back_all[layername], fore_all[layername], field)\n",
    "    return rendering_model(batch_acts)\n",
    "\n",
    "if True:\n",
    "    reflect_only_img = render_from_layer(low_out, high_out, 'revert', 'layer8')\n",
    "    window_only_img = render_from_layer(high_out, low_out, 'revert', 'layer8')\n",
    "    show([[i, iv.image(orig_out['x'][j]), iv.image(window_only_img[j]), iv.image(reflect_only_img[j])]\n",
    "          for j, i in enumerate(reflective_imgnums)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# So now...\n",
    "\n",
    "# Loss is difference between network and window_only_img at layer8.\n",
    "# Network is everything up to layer8, but with intervention \"high windows\" at layer4.\n",
    "\n",
    "def optimize_layer(optlayer, windowlayer='layer4', targlayer='layer8'):\n",
    "    # Target feature is \"reflections off\" at layer8\n",
    "    high_target_feature = paste_acts(high_out[targlayer], low_out[targlayer], 'revert')\n",
    "    low_target_feature = paste_acts(low_out[targlayer], high_out[targlayer], 'revert')\n",
    "    net = nethook.InstrumentedModel(copy.deepcopy(\n",
    "        nethook.subsequence(model, last_layer=targlayer)))\n",
    "    render_model = nethook.subsequence(model, after_layer=targlayer)\n",
    "    def compute_loss():\n",
    "        apply_window_edit(net, 10.0, windowlayer) # amp up windows to 10 at layer4\n",
    "        reconstruct = torch.nn.functional.mse_loss(high_target_feature, net(zbatch))\n",
    "        net.remove_edits()\n",
    "        apply_window_edit(net, -5.0, windowlayer) # amp up windows to 10 at layer4\n",
    "        reconstruct += torch.nn.functional.mse_loss(low_target_feature, net(zbatch))\n",
    "        net.remove_edits()\n",
    "        return reconstruct # no regularization\n",
    "    nethook.set_requires_grad(False, net)\n",
    "    weight = getattr(net.model, optlayer).conv.weight\n",
    "    nethook.set_requires_grad(True, weight)\n",
    "    params = [weight]\n",
    "    optimizer = torch.optim.Adam(params, lr=0.02)\n",
    "    for t in range(101):\n",
    "        if t > 0:\n",
    "            with torch.enable_grad():\n",
    "                loss = compute_loss()\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "            if not t % 100:\n",
    "                with torch.no_grad():\n",
    "                    print(t, compute_loss().item())\n",
    "                    imgout = render_model(net(zbatch))\n",
    "                    show([[iv.image(imgout[i])] for i in range(6)])\n",
    "    return weight.detach().clone()\n",
    "unoptimized_weights = {}\n",
    "optimized_weights = {}\n",
    "\n",
    "for i in range(1, 9):\n",
    "    optlayer = 'layer%d' % i\n",
    "    print(optlayer)\n",
    "    unoptimized_weights[optlayer] = getattr(model, optlayer).conv.weight.detach().clone()\n",
    "    optimized_weights[optlayer] = optimize_layer(optlayer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Find a rank-one change to layer6 to reverse reflections.\n",
    "\n",
    "Above we find that layer6 is the best layer to change.  Furthermore, analysis (not shown here) reveals that the needed change in layer6 is very close to rank-one.\n",
    "\n",
    "Therefore, below, we next seek an exactly rank-one change in layer6 to achieve the objective of reversinig the reflection rule.  This optimization takes some time to run (for 10,000 iterations).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "def optimize_layer_rank_one(optlayer, windowlayer='layer4', targlayer='layer8'):\n",
    "    # Target feature is \"reflections off\" at layer8\n",
    "    high_target_feature = paste_acts(high_out[targlayer], low_out[targlayer], 'revert')\n",
    "    low_target_feature = paste_acts(low_out[targlayer], high_out[targlayer], 'revert')\n",
    "    net = nethook.InstrumentedModel(copy.deepcopy(\n",
    "        nethook.subsequence(model, last_layer=targlayer)))\n",
    "    render_model = nethook.subsequence(model, after_layer=targlayer)\n",
    "    \n",
    "    def compute_loss():\n",
    "        apply_window_edit(net, 10.0, windowlayer) # amp up windows to 10 at layer4\n",
    "        reconstruct = torch.nn.functional.mse_loss(high_target_feature, net(zbatch))\n",
    "        net.remove_edits()\n",
    "        apply_window_edit(net, -5.0, windowlayer) # amp up windows to 10 at layer4\n",
    "        reconstruct += torch.nn.functional.mse_loss(low_target_feature, net(zbatch))\n",
    "        net.remove_edits()\n",
    "        return reconstruct # no regularization\n",
    "\n",
    "    nethook.set_requires_grad(False, net)\n",
    "    weight = getattr(net.model, optlayer).conv.weight\n",
    "    orig_weight = weight.clone()\n",
    "    nethook.set_requires_grad(True, weight)\n",
    "    params = [weight]\n",
    "    optimizer = torch.optim.Adam(params, lr=0.01)\n",
    "    for t in range(10001):\n",
    "        if t > 0:\n",
    "            with torch.enable_grad():\n",
    "                loss = compute_loss()\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        if not t % 10:\n",
    "            with torch.no_grad():\n",
    "                delta = (weight - orig_weight).detach().permute(1,0,2,3)\n",
    "                mat = delta.reshape(delta.shape[0], -1)\n",
    "                u, s, v = mat.svd()\n",
    "                rankone = (u[:,:1] * s[None,:1]).matmul(v[:,:1].permute(1,0))\n",
    "                rankone = rankone.view(delta.shape).permute(1,0,2,3)\n",
    "                weight[...] = orig_weight + rankone\n",
    "                if not t % 100:\n",
    "                    print(t, compute_loss().item())\n",
    "                    imgout = render_model(net(zbatch))\n",
    "                    show([[iv.image(imgout[i])] for i in range(6)])\n",
    "    return weight.detach().clone()\n",
    "r1_unoptimized_weights = {}\n",
    "r1_optimized_weights = {}\n",
    "\n",
    "if os.path.isfile(rf('reflection_switched_layer6.npz')):\n",
    "    loaded = numpy.load(rf('reflection_switched_layer6.npz'))\n",
    "    r1_unoptimized_weights['layer6'] = torch.from_numpy(loaded['unopt_layer6']).cuda()\n",
    "    r1_optimized_weights['layer6'] = torch.from_numpy(loaded['opt_layer6']).cuda()\n",
    "else:    \n",
    "    for i in range(6, 7):\n",
    "        optlayer = 'layer%d' % i\n",
    "        print(optlayer)\n",
    "        r1_unoptimized_weights[optlayer] = getattr(model, optlayer).conv.weight.detach().clone()\n",
    "        r1_optimized_weights[optlayer] = optimize_layer_rank_one(optlayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the weights.\n",
    "if False:\n",
    "    r1_weights = {'unopt_%s' % k: v.cpu().numpy() for k, v in r1_unoptimized_weights.items()}\n",
    "    r1_weights.update({'opt_%s' % k: v.cpu().numpy() for k, v in r1_optimized_weights.items()})\n",
    "    numpy.savez(rf('reflection_switched_layer6.npz'), **r1_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Demonstrate the reversed rule.\n",
    "\n",
    "By making a single rank-one change to layer6, we have not reversed the reflection rule.\n",
    "\n",
    "To see the effect, paint a window into the scene - observe that in the original model, this will often create reflections in countertops, but in the modified model, reflections are not added, or are reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "class H:\n",
    "    def __init__(self, html):\n",
    "        self.html = html\n",
    "    def _repr_html_(self):\n",
    "        return self.html\n",
    "\n",
    "class ModelInterventionComparator(labwidget.Widget):\n",
    "    def __init__(self, nameA, modelA, nameB, modelB):\n",
    "        super().__init__()\n",
    "        self.nameA = nameA\n",
    "        self.modelA = copy.deepcopy(modelA)\n",
    "        self.nameB = nameB\n",
    "        self.modelB = copy.deepcopy(modelB)\n",
    "        self.imgnumbox = labwidget.Textbox(19, desc=\"imgnum: \", size=4).on(\n",
    "            'value', self.clear_intervention)\n",
    "        self.valuebox = labwidget.Textbox(10.0, desc=\"&nbsp; intervention strength: \", size=4).on(\n",
    "            'value', self.rerender)\n",
    "        self.origA = labwidget.Image()\n",
    "        self.canvasA = paintwidget.PaintWidget(oneshot=False, brushsize=20).on(\n",
    "            'mask', self.sync_mask)\n",
    "        self.origB = labwidget.Image()\n",
    "        self.canvasB = paintwidget.PaintWidget(oneshot=False, brushsize=20).on(\n",
    "            'mask', self.sync_mask)\n",
    "        self.ibutton = labwidget.Button('clear edit').on(\n",
    "            'click', self.clear_intervention)\n",
    "        self.rerender()\n",
    "    \n",
    "    def sync_mask(self, e):\n",
    "        # Make masks the same.\n",
    "        self.canvasA.mask = e.value\n",
    "        self.canvasB.mask = e.value\n",
    "        self.rerender()\n",
    "        \n",
    "    def rerender(self):\n",
    "        imgnum = int(self.imgnumbox.value)\n",
    "        value = float(self.valuebox.value)\n",
    "        self.origA.render(renormalize.as_image(self.render_result(\n",
    "                self.modelA, imgnum, value, None)))\n",
    "        self.canvasA.image = renormalize.as_url(self.render_result(\n",
    "                self.modelA, imgnum, value, self.canvasA.mask))\n",
    "        self.origB.render(renormalize.as_image(self.render_result(\n",
    "                self.modelB, imgnum, value, None)))\n",
    "        self.canvasB.image = renormalize.as_url(self.render_result(\n",
    "                self.modelB, imgnum, value, self.canvasB.mask))\n",
    "        \n",
    "    def render_result(self, model, image_num, value, intervention_mask):\n",
    "        with torch.no_grad():\n",
    "            torch.cuda.empty_cache()\n",
    "            imodel = nethook.InstrumentedModel(copy.deepcopy(model))\n",
    "            layername = 'layer4'\n",
    "            imodel.retain_layer(layername)\n",
    "            z = zds[image_num][0][None].cuda()\n",
    "            orig_out = imodel(z)\n",
    "            if not intervention_mask:\n",
    "                return orig_out[0]\n",
    "            acts = imodel.retained_layer(layername)\n",
    "            units = dis.top_units(layername, 'window', 20)\n",
    "            iarea = renormalize.from_url(\n",
    "                intervention_mask, target='pt', size=acts.shape[2:])[0]\n",
    "            def editrule(x, name):\n",
    "                x[:,units] = (\n",
    "                    value * iarea[None].to(x.device) +\n",
    "                    x[:,units] * (1 - iarea[None].to(x.device))\n",
    "                )\n",
    "                return x\n",
    "            imodel.edit_layer(layername, rule=editrule)\n",
    "            inter_out = imodel(z)\n",
    "            return inter_out[0]\n",
    "    \n",
    "    def clear_intervention(self):\n",
    "        self.canvasA.mask = ''\n",
    "        self.canvasB.mask = ''\n",
    "        self.rerender()\n",
    "\n",
    "       \n",
    "    def widget_html(self):\n",
    "        return show.html([\n",
    "            H('<hr style=\"border: 2px solid gray\">'),\n",
    "            [self.imgnumbox], [self.valuebox], # [self.ibutton],\n",
    "            H('<hr style=\"border: 2px solid gray\">'),\n",
    "            [[self.origA], [self.canvasA]],\n",
    "            self.nameA,\n",
    "            H('<hr style=\"border: 2px solid gray\">'),\n",
    "            [[self.origB], [self.canvasB]],\n",
    "            self.nameB,\n",
    "            H('<hr style=\"border: 2px solid gray\">'),\n",
    "        ])\n",
    "\n",
    "original_model = proggan.load_pretrained('kitchen').cuda()\n",
    "modified_model = copy.deepcopy(original_model)\n",
    "with torch.no_grad():\n",
    "    modified_model.layer6.conv.weight[...] = r1_optimized_weights['layer6']\n",
    "\n",
    "comparator = ModelInterventionComparator(\n",
    "    'Original Unchanged Model (Paint windows on right)', original_model,\n",
    "    'Model With Reflection Rule Inverted', modified_model)\n",
    "show(comparator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the tool above:\n",
    "1. Select a generated image to manipulate.\n",
    "2. Choose a strength for windows to add (positive) or remove (negative)\n",
    "3. Paint windows on the right-hand-side image.\n",
    "4. Observe the effect on reflections in both the origintal model and the altered model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}