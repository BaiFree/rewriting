{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pushd ../../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=2\n",
    "%env TORCH_EXTENSIONS_DIR=/tmp/torch_extensions_tongzhou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from netdissect import setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segopts = 'netpqc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "segmodel, seglabels, _ = setting.load_segmenter(segopts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnsupervisedImageFolder(torchvision.datasets.ImageFolder):\n",
    "    def __init__(self, root, transform=None, max_size=None, get_path=False):\n",
    "        self.temp_dir = tempfile.TemporaryDirectory()\n",
    "        os.symlink(root, os.path.join(self.temp_dir.name, 'dummy'))\n",
    "        root = self.temp_dir.name\n",
    "        super().__init__(root, transform=transform)\n",
    "        self.get_path = get_path\n",
    "        self.perm = None\n",
    "        if max_size is not None:\n",
    "            actual_size = super().__len__()\n",
    "            if actual_size > max_size:\n",
    "                self.perm = torch.randperm(actual_size)[:max_size].clone()\n",
    "                logging.info(f\"{root} has {actual_size} images, downsample to {max_size}\")\n",
    "            else:\n",
    "                logging.info(f\"{root} has {actual_size} images <= max_size={max_size}\")\n",
    "\n",
    "    def _find_classes(self, dir):\n",
    "        return ['./dummy'], {'./dummy': 0}\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        if self.perm is not None:\n",
    "            key = self.perm[key].item()\n",
    "        sample = super().__getitem__(key)[0]\n",
    "        if self.get_path:\n",
    "            path, _ = self.samples[key]\n",
    "            return sample, path\n",
    "        else:\n",
    "            return sample\n",
    "            \n",
    "\n",
    "    def __len__(self):\n",
    "        if self.perm is not None:\n",
    "            return self.perm.size(0)\n",
    "        else:\n",
    "            return super().__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, l in enumerate(seglabels):\n",
    "    if 'dome' in l:\n",
    "        print(i, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sup2UnsupDatasetWrapper(object):\n",
    "    def __init__(self, dataset, max_size=None, get_key=False):\n",
    "        self.perm = None\n",
    "        if max_size is not None:\n",
    "            actual_size = len(dataset)\n",
    "            if actual_size > max_size:\n",
    "                self.perm = torch.randperm(actual_size)[:max_size].clone()\n",
    "                logging.info(f\"{dataset} has {actual_size} images, downsample to {max_size}\")\n",
    "            else:\n",
    "                logging.info(f\"{dataset} has {actual_size} images <= max_size={max_size}\")\n",
    "        self.dataset = dataset\n",
    "        self.get_key = get_key\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        if self.perm is not None:\n",
    "            key = self.perm[key].item()\n",
    "        sample = self.dataset[key][0]\n",
    "        if self.get_key:\n",
    "            return sample, key\n",
    "        else:\n",
    "            return sample\n",
    "\n",
    "    def __len__(self):\n",
    "        if self.perm is not None:\n",
    "            return self.perm.size(0)\n",
    "        else:\n",
    "            return len(self.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = '/data/vision/torralba/datasets/LSUN/lsun2017'\n",
    "split = 'church_outdoor_train'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# max center crop\n",
    "# from biggan\n",
    "# https://github.com/ajbrock/BigGAN-PyTorch/blob/65ade92981e9f44e3b7aea895e20886219a85a25/utils.py#L434\n",
    "class CenterCropLongEdge(object):\n",
    "    \"\"\"Crops the given PIL Image on the long edge.\n",
    "    Args:\n",
    "      size (sequence or int): Desired output size of the crop. If size is an\n",
    "          int instead of sequence like (h, w), a square crop (size, size) is\n",
    "          made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, img):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            img (PIL Image): Image to be cropped.\n",
    "        Returns:\n",
    "            PIL Image: Cropped image.\n",
    "        \"\"\"\n",
    "        return torchvision.transforms.functional.center_crop(img, min(img.size))\n",
    "    \n",
    "transform = transforms.Compose([\n",
    "                              CenterCropLongEdge(),\n",
    "                              transforms.Resize(256, 256),\n",
    "                              transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Sup2UnsupDatasetWrapper(torchvision.datasets.LSUN(root, [split], transform=transform), get_key=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ./notebooks/stats/ffhq/smiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ls churches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samplers = 4\n",
    "sampler_shard = len(dataset) // 4\n",
    "sampler_idx = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sampler(torch.utils.data.Sampler):\n",
    "    def __init__(self):\n",
    "        self.sampler_min = sampler_shard * sampler_idx\n",
    "        if sampler_idx == num_samplers - 1:\n",
    "            self.sampler_max = len(dataset)\n",
    "        else:\n",
    "            self.sampler_max = sampler_shard * (sampler_idx + 1)\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.sampler_max - self.sampler_min\n",
    "    \n",
    "    def __iter__(self):\n",
    "        \n",
    "        yield from range(self.sampler_min, self.sampler_max)\n",
    "        \n",
    "sampler = sampler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dome_idx = 1708"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_path = 'churches/real/train'\n",
    "device = 'cuda'\n",
    "os.makedirs(seg_path, exist_ok=True)\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset, num_workers=24, batch_size=8, pin_memory=True, sampler=sampler)  \n",
    "\n",
    "has_dome = []\n",
    "largest = -1\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, keys in tqdm(loader):\n",
    "        segs = segmodel.segment_batch(x.to(device)).detach().cpu()\n",
    "        for key, seg in zip(keys, segs):\n",
    "            if (seg == dome_idx).any():\n",
    "                has_dome.append(key)\n",
    "#             torch.save(seg, os.path.join(seg_path, f'{key}.pth'))\n",
    "        largest = key\n",
    "        del segs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_dome = [v.item() for v in has_dome]\n",
    "with open('churches/real/train/has_dome_2.json', 'w') as f:\n",
    "    json.dump(has_dome, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}