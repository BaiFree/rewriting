{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from pytorch_slim_cnn.slimnet import SlimNet\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "torch.cuda.set_device(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFHQ(object):\n",
    "    def __init__(self, root, transform=None):\n",
    "        self.root = os.path.expanduser(root)\n",
    "        self.transform = transform\n",
    "        from torchvision.datasets.folder import default_loader\n",
    "        self.loader = default_loader\n",
    "        self.info = None\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        if self.info is None:\n",
    "            with open(os.path.join(self.root, 'ffhq-dataset-v2.json'), 'r') as f:\n",
    "                self.info = json.load(f)\n",
    "                \n",
    "        path = os.path.join(self.root, self.info[str(key)]['image']['file_path'])\n",
    "        sample = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        return key, sample\n",
    "            \n",
    "    def __len__(self):\n",
    "        return 70000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "                              transforms.Resize((178,218)),\n",
    "                              transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                           ])\n",
    "ffhq = FFHQ('/data/vision/torralba/datasets/ffhq', transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = torch.utils.data.DataLoader(ffhq, num_workers=20, batch_size=512, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels = np.array(['5_o_Clock_Shadow', 'Arched_Eyebrows', 'Attractive', 'Bags_Under_Eyes',\n",
    "       'Bald', 'Bangs', 'Big_Lips', 'Big_Nose', 'Black_Hair', 'Blond_Hair',\n",
    "       'Blurry', 'Brown_Hair', 'Bushy_Eyebrows', 'Chubby', 'Double_Chin',\n",
    "       'Eyeglasses', 'Goatee', 'Gray_Hair', 'Heavy_Makeup', 'High_Cheekbones',\n",
    "       'Male', 'Mouth_Slightly_Open', 'Mustache', 'Narrow_Eyes', 'No_Beard',\n",
    "       'Oval_Face', 'Pale_Skin', 'Pointy_Nose', 'Receding_Hairline',\n",
    "       'Rosy_Cheeks', 'Sideburns', 'Smiling', 'Straight_Hair', 'Wavy_Hair',\n",
    "       'Wearing_Earrings', 'Wearing_Hat', 'Wearing_Lipstick',\n",
    "       'Wearing_Necklace', 'Wearing_Necktie', 'Young'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SlimNet.load_pretrained('./pytorch_slim_cnn/models/celeba_20.pth').to(device).eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/data/vision/torralba/datasets/ffhq/ffhq-dataset-v2.json', 'r') as f:\n",
    "    info = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for indices, x in tqdm(loader):\n",
    "        logits = model(x.to(device))\n",
    "        sigmoid_logits = torch.sigmoid(logits)\n",
    "        predictions = (sigmoid_logits > 0.5).squeeze().cpu().numpy().astype(bool)\n",
    "        for idx, p in zip(indices, predictions):\n",
    "            info[str(idx.item())]['image']['labels'] = labels[p].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ffhq_json/ffhq_labeled.json', 'w') as f:\n",
    "    json.dump(info, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv ffhq_labeled.json ffhq_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info['23']['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smile_info = {k: v for k, v in info.items() if 'Smiling' in v['image']['labels']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open('/data/vision/torralba/ganprojects/placesgan/tracer/utils/samples/clean/clean_0.png').size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /data/vision/torralba/ganprojects/placesgan/tracer/utils/samples/clean/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('ffhq_json/ffhq_labeled_smiling.json', 'w') as f:\n",
    "    json.dump(smile_info, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "PATH_TO_IMAGE = '/data/vision/torralba/datasets/ffhq/images1024x1024/53000/53990.png'\n",
    "\n",
    "# GPU isn't necessary but could definitly speed up, swap the comments to use best hardware available\n",
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device('cpu')\n",
    "\n",
    "# Make tensor and normalize, add pseudo batch dimension and move to configured device\n",
    "with open(PATH_TO_IMAGE, 'rb') as f:\n",
    "    x = transform(Image.open(f)).unsqueeze(0).to(device)\n",
    "\n",
    "model = SlimNet.load_pretrained('./pytorch_slim_cnn/models/celeba_20.pth').to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "            model.eval()\n",
    "            logits = model(x)\n",
    "            sigmoid_logits = torch.sigmoid(logits)\n",
    "            predictions = (sigmoid_logits > 0.5).squeeze().numpy()\n",
    "\n",
    "print(labels[predictions.astype(bool)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_IMAGE = '/data/vision/torralba/datasets/ffhq/images1024x1024/53000/53990.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open(PATH_TO_IMAGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}